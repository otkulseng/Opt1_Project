\section{Cable net structures}
In this section, we are analysing the situation where all members of the structure are cables, and where we fix certain nodes in order to ensure a solution exists.

This gives us the following optimization problem:

\begin{equation}
    \underset{X}{\text{min }} E(X) = \sumset{E} \ece + \ee \quad \text{s.t. } x^{(i)} = p^{(i)}, i = 1,...,M
    \label{cableNet}
\end{equation}

In order to optimize this function with an algorithm that fits the problem, it will be useful to know wether the function is continious, $C^1$, $C^2$.

We have already shown that a more general problem is continious, therefore \eqref{cableNet} is continious. The gradient is 
\begin{equation}
    \nabla \sum_{i=1}^N m_i g x_3^{(i)}
    = (0,0,m_1 g, 0,0,m_2 g,...,0,0,m_N g)
    \label{gradient_external_force}
\end{equation}
It's clear that $\ee = \sum_{i=1}^N m_i g x_3^{(i)} \in C^{\infty}$. 

As $\ece$ is a piecewise function, we need to ensure that the derivative at $\xnorm = \el$ is equal to zero. When $\xnorm < \el$ the gradient is always $\Vec{0}$, so we will only consider $\xnorm>\el$

\begin{align}
    \nabla \ece =  &\frac{k}{\el^2}(\xnorm - \el) \nabla (\sqrt{ \sum_{k= 1,2,3}(x_k^{(i)}-x_k^{(j)})^2}-\el)\\&=\frac{k}{\el^2} (\xnorm - \el) \frac{1}{2\sqrt{\sum_{k=1,2,3}(x_k^{(i)}-x_k^{(j)})^2}}\nabla \sum_{k=1,2,3}(x_k^{(i)}-x_k^{(j)})^2\\ &= \frac{k}{\el^2} (1 - \frac{\el}{\xnorm}) \frac{1}{2} 2(x_k^{(i)} - x_k^{(j)}) \nabla (x_k^{(i)} - x_k^{(j)})\\ &= \frac{k}{\el^2} (1- \frac{\el}{\xnorm}) (x_k^{(i)} - x_k^{(j)}) ( 1, 1, 1,-1,-1,-1)
\end{align}
Note that $\xnorm>\el$, which means that we never divide by zero. We see that inserting $\xnorm = \el$ we get $\vec{0}$, which shows that the expression is $C^1$. Thus, a sum of $C^1$ functions are $C^1$.

\textbf{Mangler godt argument for hvorfor den ikke er $C^2$.}
Note that the problem is typically not $C^2$ because that would require that the derivative above is continious, which it clearly is not unless
$\sumset{E} \ece = 0$, which is only possible if we have $\mathcal{E} = \emptyset$, or cables with $0$ energy, and these are not particularly interesting situations.

\subsection{Convexity}

Convexity is another property that is of great importance when considering the choice of optimization algorithm.  $\ee =\sum_{i=1}^N m_i g x_3^{(i)}$ is convex, but not strictly convex:
\begin{equation*}
    E(\lambda X +(1-\lambda) Y) = \sum_{i=1}^N m_i g (\lambda x_3^{(i)} + (1-\lambda) y_3^{(i)})
    =\sum_{i=1}^N \lambda m_i g x_3^{(i)} + (1-\lambda) m_i g  y_3^{(i)} = \lambda E(X) + (1-\lambda) E(Y)
\end{equation*}
For the elastic energy of the cable, we will consider a given edge $e_{ij}$ where we assume that $\xnorm > \el$. If $\xnorm \leq \el$, we can simply ignore the node as it does not have any energy. Note that this assumes that the mass of node $i$ and $j$ are non-zero.
\begin{align*}
     E^{cable}_{elast}(\lambda e_{ij} + (1-\lambda) \tilde{e_{ij}}) = \frac{k}{2 \el^2} \bigg( \lVert \lambda(x^{(i)}-x^{(j)}) + ( 1-\lambda ) (\tilde{x}^{(i)}-\tilde{x}^{(j)} )\rVert -\el\bigg)^2\\ \leq \frac{k}{2 \el^2} \bigg( \lambda \lVert (x^{(i)}-x^{(j)}) \rVert +( 1-\lambda ) \lVert (\tilde{x}^{(i)}-\tilde{x}^{(j)} \rVert -\el\bigg)^2 \\ <
     \frac{k}{2 \el^2}\bigg( \lambda\lVert  (x^{(i)}-x^{(j)}) \rVert - \el +( 1-\lambda ) ( \lVert \tilde{x}^{(i)}-\tilde{x}^{(j)} \rVert -\el ) \bigg)^2 \quad \text{if $\el > 0$} \\
     \leq \lambda \frac{k}{2 \el^2}\bigg( \lVert  (x^{(i)}-x^{(j)}) \rVert - \el \bigg)^2 + ( 1-\lambda )\frac{k}{2 \el^2}\bigg( ( \lVert \tilde{x}^{(i)}-\tilde{x}^{(j)} \rVert -\el ) \bigg)^2 \\= \lambda E^{cable}_{elast}(e_{ij}) + (1-\lambda) E^{cable}_{elast}(\Tilde{e_{ij}})
\end{align*}

We see that the function is strictly convex. From this, we have that the energy function is convex as the sum of a convex and strictly convex function is strictly convex. 

The fact that this energy expression is convex means that we can use Quasi-Newton methods. These are very efficient methods. We choose to avoid using Newton's Method because calculating the Hessian is cumbersome.

\subsection{Neccesary and sufficient optimality conditions}
As we have a convex function that is differentiable, the neccesary and sufficient optimality condition is simply \begin{equation}
    \nabla E(X) = 0
\end{equation}
This will be a global minimizer, again due to convexity.